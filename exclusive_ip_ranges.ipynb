{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "1. Notebook holds code for:\n",
    "2. generating ip ranges\n",
    "3. creating connection between jdbc db(write and read)\n",
    "4. two solutions for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator for ip ranges \n",
    "Generator is builed with python and creates ranges that can have length between 1 and 100. At the end csv file with data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import random\n",
    "import socket\n",
    "import struct\n",
    "socket.inet_ntoa(struct.pack('>I', random.randint(1, 0xffffffff)))\n",
    "import csv  \n",
    "fields = ['id', 'start', 'end']  \n",
    "filename = \"ips.csv\"\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "\n",
    "def int2ip(addr):\n",
    "    return socket.inet_ntoa(struct.pack(\"!I\", addr))\n",
    "\n",
    "def generate_ips_csv(number_of_ranges):\n",
    "    with open(filename, 'w') as csvfile:  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "\n",
    "        csvwriter.writerow(fields)  \n",
    "\n",
    "        for i in range(number_of_ranges):\n",
    "            startIp = socket.inet_ntoa(struct.pack('>I', random.randint(1, 0xffffffff)))\n",
    "\n",
    "            randomRange = random.randint(1, 100)\n",
    "\n",
    "            endIp = int2ip(ip2int(startIp)+randomRange)\n",
    "            row = [i, startIp, endIp]\n",
    "            csvwriter.writerow(row) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "generate_ips_csv(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://3c68ebd29f52:4041\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1614337871675)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 0\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x:Int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of properties for db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Properties\n",
       "fileNameWithData: String = ips.csv\n",
       "url: String = jdbc:mysql://mysql/test\n",
       "driver: String = com.mysql.jdbc.Driver\n",
       "dbtable: String = test\n",
       "user: String = user\n",
       "password: String = password\n",
       "props: java.util.Properties = {password=password, driver=com.mysql.jdbc.Driver, user=user}\n",
       "res0: Object = null\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Properties\n",
    "\n",
    "val fileNameWithData = \"ips.csv\"\n",
    "val url = \"jdbc:mysql://mysql/test\"\n",
    "val driver = \"com.mysql.jdbc.Driver\"\n",
    "val dbtable = \"test\"\n",
    "val user = \"user\"\n",
    "val password = \"password\"\n",
    "\n",
    "val props = new Properties()\n",
    "props.setProperty(\"user\", user)\n",
    "props.setProperty(\"password\", password)\n",
    "props.setProperty(\"driver\", driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv file and save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "33: error: not found: value generate_ips_csv",
     "output_type": "error",
     "traceback": [
      "<console>:33: error: not found: value generate_ips_csv",
      "       generate_ips_csv(10)",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SaveMode}\n",
    "val ipAddresses = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"ips.csv\").toDF(\"id\",\"start\",\"end\")\n",
    "ipAddresses.write.mode(SaveMode.Overwrite).option(\"truncate\", true).jdbc(url,dbtable, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| id|          start|            end|\n",
      "+---+---------------+---------------+\n",
      "|  0|  122.42.253.51|  122.42.253.73|\n",
      "|  1|244.180.147.103|244.180.147.117|\n",
      "|  2|  31.186.20.154|  31.186.20.205|\n",
      "|  3|206.223.187.253| 206.223.188.34|\n",
      "|  4| 146.225.225.14| 146.225.225.38|\n",
      "|  5| 158.193.240.29| 158.193.240.40|\n",
      "|  6|    45.9.81.100|    45.9.81.162|\n",
      "|  7|  35.207.18.138|  35.207.18.157|\n",
      "|  8|  199.92.142.96| 199.92.142.184|\n",
      "|  9|218.144.237.124|218.144.237.138|\n",
      "| 10|  43.198.90.187|   43.198.91.21|\n",
      "| 11| 251.36.222.176| 251.36.222.180|\n",
      "| 12|139.235.162.138|139.235.162.161|\n",
      "| 13| 240.173.235.61|240.173.235.139|\n",
      "| 14|102.216.137.164|102.216.137.192|\n",
      "| 15| 205.220.55.115| 205.220.55.182|\n",
      "| 16|  195.181.63.87| 195.181.63.138|\n",
      "| 17|120.107.180.100|120.107.180.131|\n",
      "| 18|  49.155.121.53|  49.155.121.85|\n",
      "| 19| 117.69.238.170|  117.69.239.11|\n",
      "+---+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ipAddressesDataframe: org.apache.spark.sql.DataFrame = [id: string, start: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ipAddressesDataframe = spark.read.jdbc(url,dbtable, props)\n",
    "ipAddressesDataframe.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipToLong: (ipAddress: String)Long\n",
       "longToIP: (long: Long)String\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ipToLong(ipAddress: String): Long = {\n",
    "   ipAddress.split(\"\\\\.\").reverse.zipWithIndex.map(a=>a._1.toInt*math.pow(256,a._2).toLong).sum\n",
    "}\n",
    "\n",
    "def longToIP(long: Long): String = {\n",
    "   (0 until 4).map(a=>long / math.pow(256, a).floor.toInt % 256).reverse.mkString(\".\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+----------+----------+\n",
      "| id|          start|            end|start_long|  end_long|\n",
      "+---+---------------+---------------+----------+----------+\n",
      "|  0|  122.42.253.51|  122.42.253.73|2049637683|2049637705|\n",
      "|  1|244.180.147.103|244.180.147.117|4105474919|4105474933|\n",
      "|  2|  31.186.20.154|  31.186.20.205| 532288666| 532288717|\n",
      "+---+---------------+---------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "udfIpToLong: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3605/0x00000008414c7840@163a183c,LongType,List(Some(class[value[0]: string])),None,true,true)\n",
       "udfLongToIp: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3609/0x00000008414d0040@59eb3dcd,StringType,List(Some(class[value[0]: bigint])),None,true,true)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val udfIpToLong = udf((s: String) => Option(s).map(str => ipToLong(str)))\n",
    "val udfLongToIp = udf((s: Long) => Option(s).map(ipLong => longToIP(ipLong)))\n",
    "\n",
    "ipAddressesDataframe\n",
    ".withColumn(\"start_long\", udfIpToLong($\"start\"))\n",
    ".withColumn(\"end_long\", udfIpToLong($\"end\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [ip_start: string, ip_end: string ... 2 more fields]\n",
       "test: scala.collection.immutable.Map[Long,Long] = Map(3318480896 -> 3318480920, 3318415360 -> 3318614527, 3387492260 -> 3387492264, 3387492256 -> 3387492264, 3387492263 -> 3387492263, 3414491392 -> 3414491408)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((\"197.203.0.0\", \"197.206.9.255\"),\n",
    "             (\"197.204.0.0\", \"197.204.0.24\"),\n",
    "             (\"201.233.7.160\", \"201.233.7.168\"),\n",
    "             (\"201.233.7.164\", \"201.233.7.168\"),\n",
    "             (\"201.233.7.167\", \"201.233.7.167\"),\n",
    "             (\"203.133.1.0\", \"203.133.1.16\")).toDF(\"ip_start\", \"ip_end\")\n",
    "            .withColumn(\"start\", udfIpToLong($\"ip_start\"))\n",
    "            .withColumn(\"end\", udfIpToLong($\"ip_end\"))\n",
    "\n",
    "val test = df.select(\"start\",\"end\").orderBy(asc(\"start\")).collect().map(r=> (r(0).toString.toLong, r(1).toString.toLong))\n",
    ".toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive approach. Create set of up all ip addresses from ranges and delete those that are in other ranges\n",
    "\n",
    "Each ip address is converted to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+----------+--------------------+\n",
      "|     ip_start|       ip_end|     start|       end|          ipAddreses|\n",
      "+-------------+-------------+----------+----------+--------------------+\n",
      "|  197.203.0.0|197.206.9.255|3318415360|3318614527|[3318415360, 3318...|\n",
      "|  197.204.0.0| 197.204.0.24|3318480896|3318480920|[3318480896, 3318...|\n",
      "|201.233.7.160|201.233.7.168|3387492256|3387492264|[3387492256, 3387...|\n",
      "|201.233.7.164|201.233.7.168|3387492260|3387492264|[3387492260, 3387...|\n",
      "+-------------+-------------+----------+----------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "range: => scala.collection.immutable.NumericRange.Inclusive[Long] <and> (start: Long, end: Long)List[Long] = NumericRange 3318480896 to 3318480920\n",
       "range: => scala.collection.immutable.NumericRange.Inclusive[Long] <and> (start: Long, end: Long)List[Long]\n",
       "udfLongToIp: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3957/0x0000000841646040@1a650640,ArrayType(LongType,false),List(Some(class[value[0]: bigint]), Some(class[value[0]: bigint])),None,true,true)\n",
       "ipAddressesPull: org.apache.spark.sql.DataFrame = [ip_start: string, ip_end: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val range  = 3318480896L to 3318480920L\n",
    "\n",
    "def range(start: Long, end: Long): List[Long] = (start to end).toList\n",
    "\n",
    "val udfLongToIp = udf((start: Long, end: Long) => range(start, end))\n",
    "\n",
    "val ipAddressesPull = df.withColumn(\"ipAddreses\", udfLongToIp($\"start\",$\"end\"))\n",
    "\n",
    "ipAddressesPull.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "29: error: not found: value Window",
     "output_type": "error",
     "traceback": [
      "<console>:29: error: not found: value Window",
      "       val windowSpec  = Window.orderBy(\"ipLong\")",
      "                         ^",
      ""
     ]
    }
   ],
   "source": [
    "val windowSpec  = Window.orderBy(\"ipLong\")\n",
    "\n",
    "\n",
    "val mutuallyExclusive = ipAddressesPull\n",
    ".withColumn(\"ipLong\",explode_outer($\"ipAddreses\"))\n",
    ".select(\"ipLong\")\n",
    ".groupBy($\"ipLong\")\n",
    ".count\n",
    ".where($\"count\"===1)\n",
    ".withColumn(\"mutuallyExclusiveIpAddresses\",udfLongToIp($\"ipLong\"))\n",
    ".select(\"mutuallyExclusiveIpAddresses\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import java.time.LocalDate\n",
       "import spark.implicits._\n",
       "findIntervals: (arr: Seq[(Long, Long)])scala.collection.mutable.Map[String,String]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import java.time.LocalDate\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "def findIntervals(arr: Seq[(Long, Long)]): scala.collection.mutable.Map[String,String] = {\n",
    "    var m = scala.collection.mutable.Map.empty[String,String]\n",
    "    var s = -10000L\n",
    "    var max_ = -100000L\n",
    "    for((start,end)<- arr){\n",
    "\n",
    "        if (start > max_){ \n",
    "            if(s>0L && s<max_) m += (longToIP(s+1L) -> longToIP(max_)) \n",
    "            max_ = end\n",
    "            s = start\n",
    "        }\n",
    "        else{\n",
    "            if(start > s) m += (longToIP(s) -> longToIP((start-1L).toLong)) \n",
    "            if(end <=max_) s=end\n",
    "            if(end > max_ || start == s){\n",
    "                s = max_ + 1 \n",
    "                max_ = end\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (max_ != -100000) m += (longToIP(s) -> longToIP(max_))\n",
    "        \n",
    "    println(m)\n",
    "    m\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(201.233.7.160 -> 201.233.7.163, 197.203.0.0 -> 197.203.255.255, 197.204.0.25 -> 197.206.9.255, 203.133.1.0 -> 203.133.1.16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [ip_start: string, ip_end: string ... 2 more fields]\n",
       "test: scala.collection.immutable.Map[Long,Long] = Map(3318480896 -> 3318480920, 3318415360 -> 3318614527, 3387492260 -> 3387492264, 3387492256 -> 3387492264, 3387492263 -> 3387492263, 3414491392 -> 3414491408)\n",
       "data: Seq[(Long, Long)] = Vector((3318415360,3318614527), (3318480896,3318480920), (3387492256,3387492264), (3387492260,3387492264), (3387492263,3387492263), (3414491392,3414491408))\n",
       "res9: scala.collection.mutable.Map[String,String] = Map(201.233.7.160 -> 201.233.7.163, 197.203.0.0 -> 197.203.255.255, 197.204.0.25 -> 197.206.9.255, 203.133.1.0 -> 203.133.1.16)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((\"197.203.0.0\", \"197.206.9.255\"),\n",
    "             (\"197.204.0.0\", \"197.204.0.24\"),\n",
    "             (\"201.233.7.160\", \"201.233.7.168\"),\n",
    "             (\"201.233.7.164\", \"201.233.7.168\"),\n",
    "             (\"201.233.7.167\", \"201.233.7.167\"),\n",
    "             (\"203.133.1.0\", \"203.133.1.16\")).toDF(\"ip_start\", \"ip_end\")\n",
    "            .withColumn(\"start\", udfIpToLong($\"ip_start\"))\n",
    "            .withColumn(\"end\", udfIpToLong($\"ip_end\"))\n",
    "\n",
    "val test = df.select(\"start\",\"end\").collect().map(r=> (r(0).toString.toLong, r(1).toString.toLong))\n",
    ".toMap\n",
    "\n",
    "val data = test.toSeq.sortBy(_._1)\n",
    "\n",
    "findIntervals(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole flow at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "generate_ips_csv(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipAddresses: org.apache.spark.sql.DataFrame = [id: string, start: string ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ipAddresses = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"ips.csv\").toDF(\"id\",\"start\",\"end\")\n",
    "ipAddresses.write.mode(SaveMode.Overwrite).option(\"truncate\", true).jdbc(url,dbtable, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(51.163.31.182 -> 51.163.32.16, 95.197.236.116 -> 95.197.236.172, 21.32.134.11 -> 21.32.134.24, 175.130.32.191 -> 175.130.32.249, 218.74.66.218 -> 218.74.67.29, 248.117.102.243 -> 248.117.103.64, 128.143.211.229 -> 128.143.211.249, 168.184.213.201 -> 168.184.214.34, 82.15.212.52 -> 82.15.212.135, 67.241.152.1 -> 67.241.152.16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ipAddresses: org.apache.spark.sql.DataFrame = [id: string, start: string ... 1 more field]\n",
       "df: org.apache.spark.sql.DataFrame = [id: string, start: string ... 3 more fields]\n",
       "test: scala.collection.immutable.Map[Long,Long] = Map(2944540862 -> 2944540921, 354453002 -> 354453016, 4168443635 -> 4168443712, 3662299865 -> 3662299933, 1139906560 -> 1139906576, 2156909540 -> 2156909561, 1376769075 -> 1376769159, 866328501 -> 866328592, 2830685640 -> 2830685730, 1606806643 -> 1606806700)\n",
       "data: Seq[(Long, Long)] = Vector((354453002,354453016), (866328501,866328592), (1139906560,1139906576), (1376769075,1376769159), (1606806643,1606806700), (2156909540,2156909561), (2830685640,2830685730), (2944540862,2944540921), (3662299865,3662299933), (4168443635,4168443712))\n",
       "res12: scala.collection.mutable.Ma...\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ipAddresses = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"ips.csv\").toDF(\"id\",\"start\",\"end\")\n",
    "ipAddresses.write.mode(SaveMode.Overwrite).option(\"truncate\", true).jdbc(url,dbtable, props)\n",
    "\n",
    "val df = ipAddressesDataframe\n",
    ".withColumn(\"start_long\", udfIpToLong($\"start\"))\n",
    ".withColumn(\"end_long\", udfIpToLong($\"end\"))\n",
    "\n",
    "val test = df.select(\"start_long\",\"end_long\").collect().map(r=> (r(0).toString.toLong, r(1).toString.toLong))\n",
    ".toMap\n",
    "\n",
    "val data = test.toSeq.sortBy(_._1)\n",
    "\n",
    "findIntervals(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
