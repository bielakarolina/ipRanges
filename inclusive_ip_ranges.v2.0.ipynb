{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Notebook holds code for:\n",
    "1. generating ip ranges\n",
    "2. creating connection between jdbc db(write and read)\n",
    "3. solution for the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator for ip ranges \n",
    "Generator is builed with python and creates ranges that can have length between 1 and 100. At the end csv file with data is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import random\n",
    "import socket\n",
    "import struct\n",
    "socket.inet_ntoa(struct.pack('>I', random.randint(1, 0xffffffff)))\n",
    "import csv  \n",
    "fields = ['id', 'start', 'end']  \n",
    "filename = \"ips.csv\"\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "\n",
    "def ip2int(addr):\n",
    "    return struct.unpack(\"!I\", socket.inet_aton(addr))[0]\n",
    "\n",
    "\n",
    "def int2ip(addr):\n",
    "    return socket.inet_ntoa(struct.pack(\"!I\", addr))\n",
    "\n",
    "def generate_ips_csv(number_of_ranges):\n",
    "    with open(filename, 'w') as csvfile:  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "\n",
    "        csvwriter.writerow(fields)  \n",
    "\n",
    "        for i in range(number_of_ranges):\n",
    "            startIp = socket.inet_ntoa(struct.pack('>I', random.randint(1, 0xffffffff)))\n",
    "\n",
    "            randomRange = random.randint(1, 100)\n",
    "\n",
    "            endIp = int2ip(ip2int(startIp)+randomRange)\n",
    "            row = [i, startIp, endIp]\n",
    "            csvwriter.writerow(row) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "generate_ips_csv(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://3c68ebd29f52:4041\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1615218795351)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "x: Int = 0\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x:Int = 0\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepareIntervalIpInLongDf: (df: org.apache.spark.sql.DataFrame, startColumnName: String, endColumnName: String)org.apache.spark.sql.DataFrame\n",
       "ipToLong: (ipAddress: String)Long\n",
       "longToIP: (long: Long)String\n",
       "udfIpToLong: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$2066/0x0000000840c27040@2013f6b7,LongType,List(Some(class[value[0]: string])),None,true,true)\n",
       "udfLongToIp: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$2108/0x0000000840cd6040@202a4b9c,StringType,List(Some(class[value[0]: bigint])),None,true,true)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareIntervalIpInLongDf(df: DataFrame, startColumnName: String, endColumnName: String): DataFrame = {\n",
    "     df\n",
    "    .withColumn(startColumnName, udfIpToLong(col(startColumnName)))\n",
    "    .withColumn(endColumnName, udfIpToLong(col(endColumnName)))\n",
    "}\n",
    "\n",
    "\n",
    "def ipToLong(ipAddress: String): Long = {\n",
    "   ipAddress.split(\"\\\\.\").reverse.zipWithIndex.map(a=>a._1.toInt*math.pow(256,a._2).toLong).sum\n",
    "}\n",
    "\n",
    "def longToIP(long: Long): String = {\n",
    "   (0 until 4).map(a=>long / math.pow(256, a).floor.toInt % 256).reverse.mkString(\".\")\n",
    "}\n",
    "\n",
    "val udfIpToLong = udf((s: String) => Option(s).map(str => ipToLong(str)))\n",
    "val udfLongToIp = udf((s: Long) => Option(s).map(ipLong => longToIP(ipLong)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create set of up all ip addresses from ranges and delete those that are in other ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.lag\n",
       "import org.apache.spark.sql.expressions.WindowSpec\n",
       "range: (start: Long, end: Long)List[Long]\n",
       "udfLongIntervalToRange: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$2141/0x0000000840cec040@8fc1fd3,ArrayType(LongType,false),List(Some(class[value[0]: bigint]), Some(class[value[0]: bigint])),None,true,true)\n",
       "getMutuallyExclusiveIps: (ipDf: org.apache.spark.sql.DataFrame, identityColumnName: String, startIpColumnName: String, endIpColumnName: String)org.apache.spark.sql.DataFrame\n",
       "createIntervalsFromContinuousNumbers: (mutuallyExclusiveDf: org.apache.spark.sql.DataFrame, identityColumnName: String, startColumnName: String, endColumnName: String)org.apache.spark.sql....\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.lag\n",
    "import org.apache.spark.sql.expressions.WindowSpec \n",
    "\n",
    "def range(start: Long, end: Long): List[Long] = (start to end).toList\n",
    "\n",
    "val udfLongIntervalToRange = udf((start: Long, end: Long) => range(start, end))\n",
    "\n",
    "def getMutuallyExclusiveIps(ipDf: DataFrame, identityColumnName: String, startIpColumnName: String, endIpColumnName: String): DataFrame = {\n",
    "        \n",
    "    ipDf\n",
    "    .withColumn(\"ipAddreses\", udfLongIntervalToRange(col(startIpColumnName),col(endIpColumnName)))\n",
    "    .withColumn(identityColumnName,explode_outer($\"ipAddreses\"))\n",
    "    .select(identityColumnName)\n",
    "    .groupBy(col(identityColumnName))\n",
    "    .count\n",
    "    .where($\"count\"===1)\n",
    "}\n",
    "\n",
    "\n",
    "def createIntervalsFromContinuousNumbers(mutuallyExclusiveDf: DataFrame, identityColumnName: String, startColumnName: String, endColumnName: String): DataFrame = {\n",
    "    \n",
    "    \n",
    "    val windowSpec  = Window.orderBy(identityColumnName)\n",
    "    val windowSpecDesc  = Window.orderBy(desc(identityColumnName))\n",
    "\n",
    "    val startIpInterval = identifyContinousNumbers(mutuallyExclusiveDf, windowSpec, identityColumnName, startColumnName)\n",
    "\n",
    "    val endIpInterval = identifyContinousNumbers(mutuallyExclusiveDf, windowSpecDesc, identityColumnName, endColumnName)\n",
    "\n",
    "    startIpInterval.join(endIpInterval, Seq(\"row_no\")).select(startColumnName, endColumnName)\n",
    "}\n",
    "\n",
    "\n",
    "def identifyContinousNumbers(\n",
    "    mutuallyExclusiveDf: DataFrame, \n",
    "    windowSpec: WindowSpec,\n",
    "    identityColumnName: String, \n",
    "    intervalColumnName: String):DataFrame = {\n",
    "    \n",
    "\n",
    "    mutuallyExclusiveDf\n",
    "    .withColumn(\"contiguous_grp\", col(identityColumnName) - lag(identityColumnName, 1,0).over(windowSpec))\n",
    "    .where(abs($\"contiguous_grp\")=!=1)\n",
    "    .groupBy(\"contiguous_grp\")\n",
    "    .agg(first(identityColumnName).alias(identityColumnName))\n",
    "    .withColumn(intervalColumnName, udfLongToIp(col(identityColumnName)))\n",
    "    .withColumn(\"row_no\", row_number().over(Window.orderBy(intervalColumnName)))\n",
    "    .drop(\"contiguous_grp\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "findMutuallyExclusiveIpIntervals: (df: org.apache.spark.sql.DataFrame, startColumnName: String, endColumnName: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findMutuallyExclusiveIpIntervals(df: DataFrame, startColumnName: String, endColumnName: String): DataFrame = {\n",
    "    \n",
    "    val identityColumn = \"ipLong\"\n",
    "    \n",
    "    val prepareDf = prepareIntervalIpInLongDf(df, startColumnName, endColumnName)\n",
    "    \n",
    "    val mutuallyExlusiveIp = getMutuallyExclusiveIps(prepareDf, identityColumn, startColumnName, endColumnName)\n",
    "    \n",
    "    createIntervalsFromContinuousNumbers(mutuallyExlusiveIp, identityColumn, startColumnName, endColumnName)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|     ip_start|         ip_end|\n",
      "+-------------+---------------+\n",
      "|  197.203.0.0|197.203.255.255|\n",
      "| 197.204.0.25|  197.206.9.255|\n",
      "|201.233.7.160|  201.233.7.163|\n",
      "|  203.133.1.0|   203.133.1.16|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "startColumnName: String = ip_start\n",
       "endColumnName: String = ip_end\n",
       "df: org.apache.spark.sql.DataFrame = [ip_start: string, ip_end: string]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startColumnName = \"ip_start\"\n",
    "val endColumnName = \"ip_end\"\n",
    "\n",
    "val df = Seq((\"197.203.0.0\", \"197.206.9.255\"),\n",
    "             (\"197.204.0.0\", \"197.204.0.24\"),\n",
    "             (\"201.233.7.160\", \"201.233.7.168\"),\n",
    "             (\"201.233.7.164\", \"201.233.7.168\"),\n",
    "             (\"201.233.7.167\", \"201.233.7.167\"),\n",
    "             (\"203.133.1.0\", \"203.133.1.16\")).toDF(startColumnName, endColumnName)\n",
    "\n",
    "findMutuallyExclusiveIpIntervals(df, startColumnName, endColumnName).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole flow at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "generate_ips_csv(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set of properties for db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Properties\n",
       "fileNameWithData: String = ips.csv\n",
       "url: String = jdbc:mysql://mysql/test\n",
       "driver: String = com.mysql.jdbc.Driver\n",
       "dbtable: String = test\n",
       "user: String = user\n",
       "password: String = password\n",
       "props: java.util.Properties = {password=password, driver=com.mysql.jdbc.Driver, user=user}\n",
       "res1: Object = null\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Properties\n",
    "\n",
    "val fileNameWithData = \"ips.csv\"\n",
    "val url = \"jdbc:mysql://mysql/test\"\n",
    "val driver = \"com.mysql.jdbc.Driver\"\n",
    "val dbtable = \"test\"\n",
    "val user = \"user\"\n",
    "val password = \"password\"\n",
    "\n",
    "val props = new Properties()\n",
    "props.setProperty(\"user\", user)\n",
    "props.setProperty(\"password\", password)\n",
    "props.setProperty(\"driver\", driver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test jdbc connection. Read generated data from .csv file and save to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ipAddresses: org.apache.spark.sql.DataFrame = [id: string, start: string ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ipAddresses = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"ips.csv\").toDF(\"id\",\"start\",\"end\")\n",
    "ipAddresses.write.mode(SaveMode.Overwrite).option(\"truncate\", true).jdbc(url,dbtable, props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from db and find mutually exclusive ip intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+---------------+\n",
      "| id|          start|            end|\n",
      "+---+---------------+---------------+\n",
      "|  0|  70.62.141.155|  70.62.141.156|\n",
      "|  1|   101.149.4.71|   101.149.4.91|\n",
      "|  2|  248.85.30.228|   248.85.31.11|\n",
      "|  3|  207.45.130.89| 207.45.130.134|\n",
      "|  4|205.234.224.135|205.234.224.229|\n",
      "+---+---------------+---------------+\n",
      "\n",
      "+---------------+---------------+\n",
      "|          start|            end|\n",
      "+---------------+---------------+\n",
      "|   101.149.4.71|   101.149.4.91|\n",
      "|205.234.224.135|205.234.224.229|\n",
      "|  207.45.130.89| 207.45.130.134|\n",
      "|  248.85.30.228|   248.85.31.11|\n",
      "|  70.62.141.155|  70.62.141.156|\n",
      "+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ipDataFrame: org.apache.spark.sql.DataFrame = [id: string, start: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ipDataFrame = spark.read.jdbc(url,dbtable, props)\n",
    "ipDataFrame.show\n",
    "findMutuallyExclusiveIpIntervals(ipDataFrame, \"start\", \"end\").show\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
